# -*- coding: utf-8 -*-
"""monolayout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kYK7m0yIALyAFvbFWfTUBoPp4SBEq_RU
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/hbutsuak95/monolayout
# %cd monolayout

!cp "/content/drive/My Drive/cv2_cuda/cv2.cpython-37m-x86_64-linux-gnu.so" .

#!cp /content/monolayout/decoder.pth /content/drive/MyDrive
#!cp /content/monolayout/encoder.pth /content/drive/MyDrive
!cp /content/drive/MyDrive/decoder.pth .
!cp /content/drive/MyDrive/encoder.pth .

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/monolayout
import torch
from monolayout import model
models = {}
device = torch.device("cpu")
 
encoder_dict = torch.load("encoder.pth", map_location=device)
feed_height = encoder_dict["height"]
feed_width = encoder_dict["width"]
models["encoder"] = model.Encoder(18, feed_width, feed_height, False)
filtered_dict_enc = {
    k: v for k,
    v in encoder_dict.items() if k in models["encoder"].state_dict()}
models["encoder"].load_state_dict(filtered_dict_enc)


decoder_path = "decoder.pth"
models["decoder"] = model.Decoder(
    models["encoder"].resnet_encoder.num_ch_enc)
models["decoder"].load_state_dict(
    torch.load(decoder_path, map_location=device))


#print(models)

feed_height = encoder_dict["height"]
feed_width = encoder_dict["width"]
 
def save_onnx(model, path, iput):
    model.eval()
    with torch.no_grad():
      out = model.forward(iput)
      print(out.shape)
      torch.onnx.export(model, iput, path, opset_version=10, verbose=True, input_names=["input"], output_names=["output"])

enc_input = torch.randn(1,3,feed_height, feed_width, device='cpu')
print(enc_input.shape)
save_onnx(models["encoder"], "encoder.onnx",enc_input)

dec_input = torch.randn(1,128,8,8, device='cpu')
save_onnx(models["decoder"], "decoder.onnx",dec_input)


import cv2
en_net = cv2.dnn.readNet("encoder.onnx")
de_net = cv2.dnn.readNet("decoder.onnx")

!pwd

import cv2, numpy as np
im = cv2.imread("street2.png")
lo = cv2.dnn.blobFromImage(im, 1.0/255, (1024, 1024), (0,0,0), True) # https://pytorch.org/docs/0.2.0/_modules/torchvision/transforms.html#ToTensor

en_net.setInput(lo)
en_out = en_net.forward()
print(en_out.shape)

de_net.setInput(en_out)
de_out = de_net.forward().squeeze()
print(de_out.shape)

true_top_view = np.zeros((de_out.shape[1], de_out.shape[2]))
true_top_view[de_out[1] > de_out[0]] = 255
print(true_top_view.shape)
cv2.imwrite("out.png", true_top_view)

!python test.py --image_path "street2.png" --model_path . --type="both"