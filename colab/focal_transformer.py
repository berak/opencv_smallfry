# -*- coding: utf-8 -*-
"""Focal-Transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Xoi6LDuPMVYJHX-LofPJzfknlLTgaZU

reason for death: Exporting the operator roll to ONNX not supported
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/microsoft/Focal-Transformer
# %cd Focal-Transformer

!wget https://projects4jw.blob.core.windows.net/model/focal-transformer/imagenet1k/focal-tiny-is224-ws7.pth
!wget https://github.com/microsoft/Focal-Transformer/raw/main/configs/focal_tiny_patch4_window7_224.yaml

pip install timm

from classification.focal_transformer import FocalTransformer as focal

def build_model(config):
    model_type = config.MODEL.TYPE
    print(f"Creating model: {model_type}")
    model = eval(model_type)(
        img_size=config.DATA.IMG_SIZE,
        patch_size=config.MODEL.FOCAL.PATCH_SIZE,
        in_chans=config.MODEL.FOCAL.IN_CHANS,
        num_classes=config.MODEL.NUM_CLASSES,
        embed_dim=config.MODEL.FOCAL.EMBED_DIM,
        depths=config.MODEL.FOCAL.DEPTHS,
        num_heads=config.MODEL.FOCAL.NUM_HEADS,
        window_size=config.MODEL.FOCAL.WINDOW_SIZE,
        mlp_ratio=config.MODEL.FOCAL.MLP_RATIO,
        qkv_bias=config.MODEL.FOCAL.QKV_BIAS,
        qk_scale=config.MODEL.FOCAL.QK_SCALE,
        drop_rate=config.MODEL.DROP_RATE,
        drop_path_rate=config.MODEL.DROP_PATH_RATE,
        ape=config.MODEL.FOCAL.APE,
        patch_norm=config.MODEL.FOCAL.PATCH_NORM,
        use_shift=config.MODEL.FOCAL.USE_SHIFT, 
        expand_stages=config.MODEL.FOCAL.EXPAND_STAGES,
        expand_sizes=config.MODEL.FOCAL.EXPAND_SIZES, 
        expand_layer=config.MODEL.FOCAL.EXPAND_LAYER,         
        focal_pool=config.MODEL.FOCAL.FOCAL_POOL,     
        focal_stages=config.MODEL.FOCAL.FOCAL_STAGES, 
        focal_windows=config.MODEL.FOCAL.FOCAL_WINDOWS,                                                   
        focal_levels=config.MODEL.FOCAL.FOCAL_LEVELS,    
        use_conv_embed=config.MODEL.FOCAL.USE_CONV_EMBED, 
        use_layerscale=config.MODEL.FOCAL.USE_LAYERSCALE, 
        use_pre_norm=config.MODEL.FOCAL.USE_PRE_NORM, 
        use_checkpoint=config.TRAIN.USE_CHECKPOINT
    )
    return model

from classification.focal_transformer import FocalTransformer as focal

model = focal(img_size=224)
print (model)

import torch
crop = (224, 224)
fn = "focal.onnx"
def convert_to_onnx(net, output_name):
    input = torch.randn(1,3,crop[0], crop[1])
    input_names = ['data']
    output_names = ['output']
    net.eval()
    torch.onnx.export(net, input, output_name, verbose=False, input_names=input_names, output_names=output_names, opset_version=9)
#model.cuda()
convert_to_onnx(model, fn)