# -*- coding: utf-8 -*-
"""lipreading.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_dZklPM6q_gi6wQyvMH8WpPGJWKAFiId

num_channels must be int !

n = cv2.dnn.readNet("c:/data/dnn/lipreading.onnx")
[ERROR:0] global C:\p\opencv\modules\dnn\src\onnx\onnx_importer.cpp (1928) handleNode DNN/ONNX: ERROR during processing node with 1 inputs and 1 outputs: [ReduceMean]:(1157)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
cv2.error: OpenCV(4.5.2-pre) C:\p\opencv\modules\dnn\src\onnx\onnx_importer.cpp:1937: error: (-2:Unspecified error) in function 'handleNode'

Node [ReduceMean]:(1157) parse error: OpenCV(4.5.2-pre) C:\p\opencv\modules\dnn\src\onnx\onnx_importer.cpp:559: error: (-213:The function/feature is not implemented) Unsupported input shape of ReduceMean operation. in function 'handleNode'
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks
# %cd Lipreading_using_Temporal_Convolutional_Networks

# no, it's not a tar file
#1omzvXV_djVIW2A7I09DWMe9JR-9o_MYh
#110MsJpKreB8fwtGPTHsODSW2D_umWT0E
!wget "https://drive.google.com/uc?export=dowload&id=1dT2e6fG46MEIev7XXYjsWSdQmjCvtgQe" -O lrw_snv2x05_tcn.pth
!wget "https://drive.google.com/uc?export=dowload&id=110MsJpKreB8fwtGPTHsODSW2D_umWT0E" -O snv05x_tcn2x.pth
!ls -l

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Lipreading_using_Temporal_Convolutional_Networks
from lipreading.model import Lipreading
import torch

#model = Lipreading(backbone_type='shufflenet')
model = Lipreading(modality='video',
                   backbone_type='shufflenet',
                   width_mult=0.5,
                   tcn_options = {'kernel_size': [3], 'num_layers': 4, 'dropout': 0.2, 'width_mult':1.0, 'dwpw':False})
state = torch.load("lrw_snv2x05_tcn.pth")["model_state_dict"]

#for k in state.keys():
#  print(k, state[k].size())
model.load_state_dict( state, strict=False)
model.eval()

with torch.no_grad():
  dummy_input = torch.randn(24, 1, 5, 96, 96, device='cpu')
  out = model.forward(dummy_input)#,lengths=[24])
  print(out.shape)
  torch.onnx.export(model, dummy_input, "lipreading.onnx", opset_version=10, verbose=True, input_names=["input"], output_names=["output"])

!ls -l

import cv2
print(cv2.__version__)
cv2.dnn.readNet("lipreading.onnx")

a = [.5,.4,.3,.2]
int(a)

!cp "/content/drive/My Drive/cv2_cuda/cv2.cpython-36m-x86_64-linux-gnu.so" .



# model.py
"""
import torch
import torch.nn as nn
from lipreading.models.resnet import ResNet, BasicBlock
from lipreading.models.resnet1D import ResNet1D, BasicBlock1D
from lipreading.models.shufflenetv2 import ShuffleNetV2
from lipreading.models.tcn import MultibranchTemporalConvNet, TemporalConvNet


# -- auxiliary functions
def threeD_to_2D_tensor(x):
    n_batch, n_channels, s_time, sx, sy = x.shape
    x = x.transpose(1, 2)
    return x.reshape(n_batch*s_time, n_channels, sx, sy)


def _average_batch(x, lengths, B):
    return torch.stack( [torch.mean( x[index][:,0:i], 1 ) for index, i in enumerate(lengths)],0 )


class MultiscaleMultibranchTCN(nn.Module):
    def __init__(self, input_size, num_channels, num_classes, tcn_options, dropout, relu_type, dwpw=False):
        super(MultiscaleMultibranchTCN, self).__init__()

        self.kernel_sizes = tcn_options['kernel_size']
        self.num_kernels = len( self.kernel_sizes )

        self.mb_ms_tcn = MultibranchTemporalConvNet(input_size, num_channels, tcn_options, dropout=dropout, relu_type=relu_type, dwpw=dwpw)
        self.tcn_output = nn.Linear(num_channels[-1], num_classes)

        self.consensus_func = _average_batch

    def forward(self, x, lengths, B):
        # x needs to have dimension (N, C, L) in order to be passed into CNN
        xtrans = x.transpose(1, 2)
        out = self.mb_ms_tcn(xtrans)
        out = self.consensus_func( out, lengths, B )
        return self.tcn_output(out)


class TCN(nn.Module):
    """Implements Temporal Convolutional Network (TCN)
    __https://arxiv.org/pdf/1803.01271.pdf
    """

    def __init__(self, input_size, num_channels, num_classes, tcn_options, dropout, relu_type, dwpw=False):
        super(TCN, self).__init__()
        num_channels = [int(i) for i in num_channels]
        self.tcn_trunk = TemporalConvNet(input_size, num_channels, dropout=dropout, tcn_options=tcn_options, relu_type=relu_type, dwpw=dwpw)
        self.tcn_output = nn.Linear(num_channels[-1], num_classes)

        self.consensus_func = _average_batch

        self.has_aux_losses = False

    def forward(self, x, lengths, B):
        # x needs to have dimension (N, C, L) in order to be passed into CNN
        x = self.tcn_trunk(x.transpose(1, 2))
        x = self.consensus_func( x, lengths, B )
        return self.tcn_output(x)


class Lipreading(nn.Module):
    def __init__( self, modality='video', hidden_dim=256, backbone_type='resnet', num_classes=500,
                  relu_type='prelu', tcn_options={}, width_mult=1.0, extract_feats=False):
        super(Lipreading, self).__init__()
        self.extract_feats = extract_feats
        self.backbone_type = backbone_type
        self.modality = modality

        if self.modality == 'raw_audio':
            self.frontend_nout = 1
            self.backend_out = 512
            self.trunk = ResNet1D(BasicBlock1D, [2, 2, 2, 2], relu_type=relu_type)
        elif self.modality == 'video':
            if self.backbone_type == 'resnet':
                self.frontend_nout = 64
                self.backend_out = 512
                self.trunk = ResNet(BasicBlock, [2, 2, 2, 2], relu_type=relu_type)
            elif self.backbone_type == 'shufflenet':
                assert width_mult in [0.5, 1.0, 1.5, 2.0], "Width multiplier not correct"
                shufflenet = ShuffleNetV2( input_size=96, width_mult=width_mult)
                self.trunk = nn.Sequential( shufflenet.features, shufflenet.conv_last, shufflenet.globalpool)
                self.frontend_nout = 24
                self.backend_out = 1024 if width_mult != 2.0 else 2048
                self.stage_out_channels = shufflenet.stage_out_channels[-1]

            frontend_relu = nn.PReLU(num_parameters=self.frontend_nout) if relu_type == 'prelu' else nn.ReLU()
            self.frontend3D = nn.Sequential(
                        nn.Conv3d(1, self.frontend_nout, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False),
                        nn.BatchNorm3d(self.frontend_nout),
                        frontend_relu,
                        nn.MaxPool3d( kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)))
        else:
            raise NotImplementedError

        tcn_class = TCN if len(tcn_options['kernel_size']) == 1 else MultiscaleMultibranchTCN
        self.tcn = tcn_class( input_size=self.backend_out,
                              num_channels=[hidden_dim*len(tcn_options['kernel_size'])*tcn_options['width_mult']]*tcn_options['num_layers'],
                              num_classes=num_classes,
                              tcn_options=tcn_options,
                              dropout=tcn_options['dropout'],
                              relu_type=relu_type,
                              dwpw=tcn_options['dwpw'],
                            )

    def forward(self, x):#, lengths):
        if self.modality == 'video':
            B, C, T, H, W = x.size()
            lengths = [B]
            x = self.frontend3D(x)
            Tnew = x.shape[2]    # outpu should be B x C2 x Tnew x H x W
            x = threeD_to_2D_tensor( x )
            x = self.trunk(x)
            if self.backbone_type == 'shufflenet':
                x = x.view(-1, self.stage_out_channels)
            x = x.view(B, Tnew, x.size(1))
        elif self.modality == 'raw_audio':
            B, C, T = x.size()
            x = self.trunk(x)
            x = x.transpose(1, 2)
            lengths = [_//640 for _ in lengths]

        return x if self.extract_feats else self.tcn(x, lengths, B)

"""