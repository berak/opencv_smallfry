# -*- coding: utf-8 -*-
"""DiverseDepth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ug6P3Wp_YO_V1ylTRlse2O8aUO1BQK6S
"""

# Commented out IPython magic to ensure Python compatibility.
# broken code !!!!
!git clone https://github.com/berak/DiverseDepth
# %cd DiverseDepth

#%cd /content/DiverseDepth
!cp "/content/drive/My Drive/cv2_cuda/cv2.cpython-37m-x86_64-linux-gnu.so" .

#!mkdir
!wget https://cloudstor.aarnet.edu.au/plus/s/ixWf3nTJFZ0YE4q/download -O resnext50_32x4d.pth

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/DiverseDepth
from lib.models.diverse_depth_model import RelDepthModel
from lib.utils.net_tools import load_ckpt
from lib.core.config import cfg
print("__ ", cfg.MODEL.ENCODER)

model = RelDepthModel()
model.eval()
"""checkpoint = torch.load("resnext50_32x4d.pth")
model_state_dict_keys = model.state_dict().keys()
checkpoint_state_dict_noprefix = strip_prefix_if_present(checkpoint['model_state_dict'], "module.")
if all(key.startswith('module.') for key in model_state_dict_keys):
    model.module.load_state_dict(checkpoint_state_dict_noprefix)
else:
    model.load_state_dict(checkpoint_state_dict_noprefix)
"""
#                                lateral_resnext50_32x4d_body_stride16():

import torch

#crop = (385, 385)
crop = (256, 256)
fn = "DiverseDepth.onnx"
def convert_to_onnx(net, output_name):
    input = {"A":torch.randn(1,3,crop[0], crop[1])}
    input_names = ['data']
    output_names = ['output']
    net.eval()
    torch.onnx.export(net, input, output_name, verbose=False, input_names=input_names, output_names=output_names, opset_version=11)
model.cuda()
convert_to_onnx(model, fn)

import cv2
print(cv2.__version__)

net = cv2.dnn.readNet(fn)

import numpy as np
im = cv2.imread("girl.png")
blob = cv2.dnn.blobFromImage(im, 1/255, crop, (127,127,127), False, True)
net.setInput(blob)
res = net.forward("output")
print(res.shape)
out = np.sum(res,1)
out = out[0,:,:]
print(out.shape)
out = out + 1
out = out / 2
out = 10 ** out
#out = cv2.normalize(out,0,1,cv2.NORM_MINMAX)
print(np.min(out), np.max(out))

from google.colab.patches import cv2_imshow
cv2_imshow(out*32)
im = cv2.resize(im,crop)
cv2_imshow(im)

net.dumpToFile("monodepth.dot")

!dot monodepth.dot -Tpng -omono.png
import cv2
im = cv2.imread("mono.png")
from google.colab.patches import cv2_imshow
cv2_imshow(im)

2240/7

#@title
import torch.nn as nn
import torch


class BidirectionalLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(BidirectionalLSTM, self).__init__()
        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)
        self.linear = nn.Linear(hidden_size * 2, output_size)

    def forward(self, input):
        self.rnn.flatten_parameters()
        recurrent, _ = self.rnn(input)
        b, T, h = recurrent.size()
        recurrent = recurrent.view(b*T, h)
        output = self.linear(recurrent)  # batch_size x T x output_size
        output = output.view(b, T, -1)
        return output


class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.SequenceModeling = nn.Sequential(
            BidirectionalLSTM(512, 512, 512),
            BidirectionalLSTM(512, 512, 512))

    def forward(self, input):
        contextual_feature = self.SequenceModeling(input)
        return contextual_feature


x = torch.rand(1, 65, 512).float().cpu()
model = Model()
model.eval()
torch.onnx.export(model, x, "ocr0811_1.onnx", verbose=True)

import cv2
net = cv2.dnn.readNetFromONNX('ocr0811_1.onnx')
net.setInput(x.numpy())
#res = net.forward()
#graph(%input.1 : Float(1:33280, 65:512, 512:1),